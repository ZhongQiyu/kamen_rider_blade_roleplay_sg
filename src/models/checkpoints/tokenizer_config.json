{
  "add_prefix_space": true,
  "bos_token": "<|endoftext|>",
  "eos_token": "<|endoftext|>",
  "model_max_length": 1024,
  "special_tokens_map_file": "special_tokens_map.json",
  "tokenizer_class": "GPT2Tokenizer",
  "unk_token": "<|unk|>",
  "do_lower_case": false,
  "max_len": 512,
  "vocab_file": "vocab.txt",
  "merges_file": "merges.txt"
}
